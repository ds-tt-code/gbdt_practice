# README

## 本プロジェクトの概要

本プロジェクトは`辻井`の`GBDT`手習い用です。
本プロジェクトは下記を試します。

* アルゴリズム
  * XGBoost
  * LightGBM
  * CatBoost

## メモ

### GBDTのパラメータチューニング

GBDTはパラメータの数も多く、各アルゴリズムにおいて調整可能なパラメータが異なります。ここでは基本的なパラメータについて解説を行い、細かいパラメータについては各ライブラリのドキュメントを参照して頂くこととします。

#### `num_boost_round (n_estimators)`

木を何本作るかの設定を行います。基本的にこちらのパラメータは100,000や、1,000,000といった限りなく大きい値を設定するだけで、それ以降はチューニングを行いません。次に説明する`early_stopping`で学習が止まった段階で学習を止めるように調整します。
このパラメータで調整すると泉岡さんに怒られるので注意！！

#### `early_stopping`

ニューラルネットのモデルでも使用されるパラメータです。学習と検証のスコアを参照し、設定した値の回数分、同じスコアが続いた時点で学習を止める機能です。多くすればするほど木を多く作ることになるので学習時間が増加しますので、実験中はなるべく小さい値で実験することを推奨します。人によって調整方法は異なりますが、筆者の場合はモデルを作り始めた当初は20から始め、最後にスコアを押し上げたい時に100~500の中で100刻みにチューニングする様にしています。

#### `objective`

目的関数の設定を行います。分類問題だと基本的に`logloss`を設定します。XGBoost, LightGBM, CatBoostでこの、`logloss`の設定名が異なるので注意が必要です。サンプルコードにて、名前の違いを確認してください。  
回帰問題の場合は`RMSE`等を設定しますが、目的変数の分布によって、`poisson`や`tweedie`等に設定した方がスコアがよくなるケースも存在します。回帰問題の場合はこちらの目的関数の設定にもより注意しましょう。

#### `metric`

モデルの評価指標を設定します。一般的にはプロジェクト等で設定されている評価指標を用いますが、こちらにおいても解きたい問題の設定によっては変更した方がスコアが良くなるケースも存在します。設定された評価指標を鵜呑みにするのは避けた方が良いです。

#### `learning_rate`

XGBoostでは`eta`となっています。学習率を指し、勾配降下の細かさを設定します。筆者は`0.1`を初期で使用し、精度の底上げを実施したい場合は`0.01`など細かい数値に設定しなおします。  
細かく設定した場合、学習にかかる時間も多くなってしまいます。

#### `max_depth`

GBDTにおける**最重要パラメータ**と言っても過言ではないです。木の深さを深くすればするほど精度は向上しますがその分、過学習しやすくなります。浅くすればするほど、汎化性能は上がりますが、未学習につながりやすくなります。  
例えるならば飲み会における、**おじさんの話の長さ**とイメージしてもらうとよいでしょう。話が長ければ長いほど、いろんなお話を聞けるかもしれませんが、全てを詰め込みすぎて頭がパンクする可能性がある様なイメージをもってもらうとよいでしょう。  

LightGBMでは`-1`を設定する事が出来、各roundにおいて木の深さに制限を持たずに学習する事が可能です。しかし、この設定を行うと過学習しやすくなるので注意が必要です。

#### `num_leaves`

構成する木における葉の数を設定します。基本的には`2**(max_depth - 1)`未満に設定します。元はLightGBMのパラメータ名で、XGBoostでは`max_leaves`と名付けられています。CatBoostではGPUを用いた学習に設定した上で、`GrowthPolicy`パラメータを変更すると設定可能となります。  
LightGBMにおいてはこの`num_leaves`が最も重要なパラメータとなります。

`max_depth`がおじさんの話の長さとするなら、この`num_leaves`は**話題の広がり方**というイメージをもってもらうとイメージしやすいでしょう。沢山の話題があれば、いろんな話が聞けますが、やはり詰め込みすぎて頭がパンクしてしまうようなイメージを持つとよいでしょう。

#### `min_data_in_leaf`

LightGBM特有でかつ重要なパラメータとなっています。余計な分割を抑えるために葉にどれだけの情報を持たせるか、というイメージを持っていただけたらと思います。大きな値に設定すると木が深くなりすぎることを防ぐことが出来ますが、その分、未学習につながる可能性もあります。  
`デフォルト値は20`で、レコード数が100行であるなど少ない場合にはこのデフォルト値からだんだん下げる様に調整します。データ数が十分である場合や特徴量をかなり抽出した場合などはこの数字を10刻みであげると良いかと思います。自信がない時はデフォルト値でも大丈夫です。

#### `subsample`

各roundにおいてサンプリングを行うデータの割合を設定します。初期では`1.0`または`0.7`を設定する事が多く、過学習気味に感じたら`0.4`等の値に下げるなどのチューニングを実施します。LightGBMでは`bagging_fraction`としても設定可能です。LightGBMでは`bagging_freq`を1以上にしないと効果が発動されないので注意です。

#### `colsample_bytree`

LightGBMでは`feature_fraction`でも使用可能です。こちらは各roundにおける特徴量をサンプリングする割合を設定します。こちらも`1.0`または`0.7`から開始して過学習気味であったり、似た様な特徴量を多く生成してしまったらと感じたら`0.4`等の値に下げる等のチューニングを行います。特徴量が100ある場合、`0.7`と設定されていれば、1本の木を作るのに利用する特徴量は70までに制限されます。

#### その他参考リンク

* [LightGBM Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)
  * LightGBMにおけるパラメータチューニング方法が記載されている
* [Laurae++ Interactive Documentation](https://sites.google.com/view/lauraepp/parameters)
  * XGBoost, LightGBMにおける同一パラメータの解説、及びチューニング方法が記載されている
