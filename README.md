# README

## 本プロジェクトの概要

本プロジェクトは`辻井`の`GBDT`手習い用です。
本プロジェクトは下記を試します。

* アルゴリズム
  * XGBoost
  * LightGBM
  * CatBoost

## メモ

### GBDTのパラメータチューニング

GBDTはパラメータの数も多く、各アルゴリズムにおいて調整可能なパラメータが異なります。ここでは基本的なパラメータについて解説を行い、細かいパラメータについては各ライブラリのドキュメントを参照して頂くこととします。

#### `num_boost_round (n_estimators)`

木を何本作るかの設定を行います。基本的にこちらのパラメータは100,000や、1,000,000といった限りなく大きい値を設定するだけで、それ以降はチューニングを行いません。次に説明する`early_stopping`で学習が止まった段階で学習を止めるように調整します。
このパラメータで調整すると泉岡さんに怒られるので注意！！

#### `early_stopping`

ニューラルネットのモデルでも使用されるパラメータです。学習と検証のスコアを参照し、設定した値の回数分、同じスコアが続いた時点で学習を止める機能です。多くすればするほど木を多く作ることになるので学習時間が増加しますので、実験中はなるべく小さい値で実験することを推奨します。人によって調整方法は異なりますが、筆者の場合はモデルを作り始めた当初は20から始め、最後にスコアを押し上げたい時に100~500の中で100刻みにチューニングする様にしています。

#### `objective`

目的関数の設定を行います。分類問題だと基本的に`logloss`を設定します。XGBoost, LightGBM, CatBoostでこの、`logloss`の設定名が異なるので注意が必要です。サンプルコードにて、名前の違いを確認してください。  
回帰問題の場合は`RMSE`等を設定しますが、目的変数の分布によって、`poisson`や`tweedie`等に設定した方がスコアがよくなるケースも存在します。回帰問題の場合はこちらの目的関数の設定にもより注意しましょう。

#### `metric`

モデルの評価指標を設定します。一般的にはプロジェクト等で設定されている評価指標を用いますが、こちらにおいても解きたい問題の設定によっては変更した方がスコアが良くなるケースも存在します。設定された評価指標を鵜呑みにするのは避けた方が良いです。

#### `learning_rate`

XGBoostでは`eta`となっています。学習率を指し、勾配降下の細かさを設定します。筆者は`0.1`を初期で使用し、精度の底上げを実施したい場合は`0.01`など細かい数値に設定しなおします。  
細かく設定した場合、学習にかかる時間も多くなってしまいます。

#### `max_depth`

GBDTにおける**最重要パラメータ**と言っても過言ではないです。木の深さを深くすればするほど精度は向上しますがその分、過学習しやすくなります。浅くすればするほど、汎化性能は上がりますが、未学習につながりやすくなります。  
例えるならば飲み会における、**おじさんの話の長さ**とイメージしてもらうとよいでしょう。話が長ければ長いほど、いろんなお話を聞けるかもしれませんが、全てを詰め込みすぎて頭がパンクする可能性がある様なイメージをもってもらうとよいでしょう。  

LightGBMでは`-1`を設定する事が出来、各roundにおいて木の深さに制限を持たずに学習する事が可能です。しかし、この設定を行うと過学習しやすくなるので注意が必要です。

#### `num_leaves`

構成する木における葉の数を設定します。基本的には`2**(max_depth - 1)`未満に設定します。元はLightGBMのパラメータ名で、XGBoostでは`max_leaves`と名付けられています。CatBoostではGPUを用いた学習に設定した上で、`GrowthPolicy`パラメータを変更すると設定可能となります。  
LightGBMにおいてはこの`num_leaves`が最も重要なパラメータとなります。

`max_depth`がおじさんの話の長さとするなら、この`num_leaves`は**話題の広がり方**というイメージをもってもらうとイメージしやすいでしょう。沢山の話題があれば、いろんな話が聞けますが、やはり詰め込みすぎて頭がパンクしてしまうようなイメージを持つとよいでしょう。

#### `min_data_in_leaf`

LightGBM特有でかつ重要なパラメータとなっています。余計な分割を抑えるために葉にどれだけの情報を持たせるか、というイメージを持っていただけたらと思います。大きな値に設定すると木が深くなりすぎることを防ぐことが出来ますが、その分、未学習につながる可能性もあります。  
`デフォルト値は20`で、レコード数が100行であるなど少ない場合にはこのデフォルト値からだんだん下げる様に調整します。データ数が十分である場合や特徴量をかなり抽出した場合などはこの数字を10刻みであげると良いかと思います。自信がない時はデフォルト値でも大丈夫です。

#### `subsample`

各roundにおいてサンプリングを行うデータの割合を設定します。初期では`1.0`または`0.7`を設定する事が多く、過学習気味に感じたら`0.4`等の値に下げるなどのチューニングを実施します。LightGBMでは`bagging_fraction`としても設定可能です。LightGBMでは`bagging_freq`を1以上にしないと効果が発動されないので注意です。

#### `colsample_bytree`

LightGBMでは`feature_fraction`でも使用可能です。こちらは各roundにおける特徴量をサンプリングする割合を設定します。こちらも`1.0`または`0.7`から開始して過学習気味であったり、似た様な特徴量を多く生成してしまったらと感じたら`0.4`等の値に下げる等のチューニングを行います。特徴量が100ある場合、`0.7`と設定されていれば、1本の木を作るのに利用する特徴量は70までに制限されます。

#### その他参考リンク

* [LightGBM Parameters Tuning](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)
  * LightGBMにおけるパラメータチューニング方法が記載されている
* [Laurae++ Interactive Documentation](https://sites.google.com/view/lauraepp/parameters)
  * XGBoost, LightGBMにおける同一パラメータの解説、及びチューニング方法が記載されている

### データ確認手順

1. データが読み込めるか？（そのままread_csvできるか？)
  文字コードや余分な文字が入り込んでいないか
1. カラム名からデータが推測できないものの意味を確認する。
1. データ型を確認
  例えばintであるはずのカラムがfloatになっていたら、欠損や、小数が入っていないか。またobject型になっていたら、文字列が紛れ込んでいないかなど
1. 欠損はあるか確認
1. ユニーク数の確認（カウントエンコーディング？を使えるかも）
  ここでデータの概要を確認
1. 数値変数・カテゴリ変数を確認
1. ヘッダの有無やカラムの中身を確認する

上記内容は関数化しておく

### データ処理手順

1. 不要なカラムの除去
  欠損率の多いカラムは不要になることが多い
1. カテゴリ変数の入力間違いなどを修正
1. 異常値の補正（明らかに桁がおかしいなど）
1. 欠損処理(ここ慎重に)

### EDA(Expantory Data Analysis)手順

1. 数値変数は必ず、ヒストグラムを描画する！  
    → 結果（目的変数）の違いによって何が異なるのかを確認する  
　　カテゴリ変数は必ず棒グラフを描画する！  
　　→　目的変数が特定のカテゴリ変数の頻度に関連している可などを確認  
　　　　特に目立った項目があれば、その原因を明らかにし、学習後のモデルの結果を後処理することで推論の精度をあげていくなどに活用する。  

### モデリング手順

1. テーブルの整形
  目的変数と説明変数のテーブルをきちんと作る
1. 評価指標策定
  若手にはなかなか難しいので、よくクライアントや周りの人に確認しながら行う。
1. バリデーション設計  
  k-fold cross validationを使うかなど。例えば時系列データをランダムに抽出したらぜんぜんだめ。
1. 特徴抽出  
  要約統計量特徴量  
  ラベルエンコーディング  
  カウントエンコーディング  
  ラグ特徴量  
  ローリング特徴量  
  kaggle で勝つデータ分析の技術　必読！
1. モデルの構築
  特に理由がなければ、GBDTを最初に使っておくと吉
  パラメータ調整に慣れておくこと！
  正規分布に近い場合はロジスティック回帰や、SVMなど古典的なモデルのほうがフィットしやすい
  またデータが少ない時もロジスティック回帰を用いることもある。前処理が若干だるいらしい
1. モデルの出力の評価を行う
   AUC（ORC曲線の面積.0.5はだめだめ。1だと過学習気味。）を使うとわかりにくいので別の評価指標や、AUCのわかりやすい仮説資料を作る  
   https://blog.kikagaku.co.jp/roc-auc  
   MAPEを要求されることが多いが場合によっては適切ではない。（100円-200の差と10000-10100の差は同じではない！)
1. パラメータチューニングする
  学習率と他パラメータを同時に調整すると評価できないので、学習率は固定してチューニングしていくこと
　学習率は最後に変える
　また、影響の大きいパラメータからチューニングしていく(正則化パラメータを永遠にいじってても無駄)
　ライブラリの自動調整を使ってもよいが過学習になることも多いので注意すること
1. 分析する
  何をどの程度なぜ予測できなかったか？
1. 最終アウトプット
  最終予測結果をクライアントが使いやすい形で出力
  予測を実行するスクリプト（学習スクリプトまで渡してしまうと、クライアント側でいくらでも学習できてしまうので、注意。ここは内緒で）

### 各GBDTのモデルの特徴

|                  | XGBoost     | LightGBM | CatBoost | CatBoost(GPU) |
| -------------------- | ------- | -------- | -------- | ------------- |
| 学習時間              | △      | ◎       | △       | ◎ |
| 少ないデータのfit(1万件境?) | 〇 | △       | 〇       | ? |
| カテゴリ変数の予測     | △      | ◎       | ◎       | ◎ |
| 前処理の容易さ         | △      | ◎       | △       | △ |
| パラメータ調整の容易さ | 〇       | △      | 〇       | △ |

データが少ない場合はCatBoostを選ぶのが良いかも
